"""
CodeBLEU Evaluation Tool
Evaluates generated code correctness using CodeBLEU metric
"""

import re
from typing import List, Tuple, Optional, Dict
import subprocess
import sys
import os


class CodeBLEUEvaluator:
    """
    Evaluates code correctness using CodeBLEU metric.
    CodeBLEU combines BLEU score with code-specific metrics.
    """
    
    def __init__(self):
        """Initialize the CodeBLEU evaluator."""
        self._ensure_dependencies()
    
    def _ensure_dependencies(self):
        """Ensure required dependencies are installed."""
        try:
            import nltk
            # Try to download punkt if not available
            try:
                nltk.data.find('tokenizers/punkt')
            except LookupError:
                nltk.download('punkt', quiet=True)
        except ImportError:
            print("Warning: nltk not found. Some features may be limited.")
            print("Install with: pip install nltk")
        
        # tree-sitter is optional for enhanced AST matching
        try:
            import tree_sitter
        except ImportError:
            # tree-sitter is optional, will use fallback methods
            pass
    
    def evaluate(self, generated_code: str, reference_code: str, language: str = "python") -> Dict[str, float]:
        """
        Evaluate generated code against reference code using CodeBLEU.
        Follows the formula: CodeBLEU = 0.25 × NG + 0.25 × WNG + 0.25 × AST + 0.25 × DF
        where:
        - NG = n-gram BLEU score (standard modified BLEU)
        - WNG = weighted n-gram score (higher weight for identifiers)
        - AST = syntax/AST match score (AST similarity)
        - DF = data-flow match score (variable/data flow similarity)
        
        Args:
            generated_code: The code generated by the AI agent
            reference_code: The correct/reference code to compare against
            language: Programming language of the code
        
        Returns:
            Dictionary containing CodeBLEU score and component scores
        """
        # Normalize code (remove whitespace differences)
        gen_normalized = self._normalize_code(generated_code)
        ref_normalized = self._normalize_code(reference_code)
        
        # Calculate NG (n-gram BLEU score) - standard modified BLEU
        ng_score = self._calculate_ng_bleu(gen_normalized, ref_normalized)
        
        # Calculate WNG (weighted n-gram score) - identifier-aware
        wng_score = self._calculate_weighted_ngram(gen_normalized, ref_normalized, language)
        
        # Calculate AST (syntax/AST match score)
        ast_score = self._ast_match_score(gen_normalized, ref_normalized, language)
        
        # Calculate DF (data-flow match score)
        df_score = self._dataflow_match_score(gen_normalized, ref_normalized, language)
        
        # Calculate CodeBLEU
        codebleu_score = (
            0.25 * ng_score +
            0.25 * wng_score +
            0.25 * ast_score +
            0.25 * df_score
        )
        
        return {
            "codebleu": codebleu_score,
            "ng": ng_score,  # n-gram BLEU
            "wng": wng_score,  # weighted n-gram
            "ast": ast_score,  # AST match
            "df": df_score,  # data-flow match
            "is_correct": codebleu_score >= 0.75  # Threshold for correctness
        }
    
    def _normalize_code(self, code: str) -> str:
        """Normalize code by removing extra whitespace and comments."""
        # Remove comments
        code = re.sub(r'#.*$', '', code, flags=re.MULTILINE)
        code = re.sub(r'//.*$', '', code, flags=re.MULTILINE)
        code = re.sub(r'/\*.*?\*/', '', code, flags=re.DOTALL)
        
        # Normalize whitespace
        lines = [line.strip() for line in code.split('\n') if line.strip()]
        return '\n'.join(lines)
    
    def _tokenize_code(self, code: str) -> List[str]:
        """
        Simple tokenization of code
        Splits on whitespace and punctuation, keeping tokens separate.
        """
        # Replace common punctuation with spaces, then split
        code = re.sub(r'([(),:;{}[\]])', r' \1 ', code)
        tokens = [t for t in code.split() if t.strip()]
        return tokens
    
    def _calculate_ng_bleu(self, generated: str, reference: str) -> float:
        """
        Implements BLEU with unigram and bigram precision, geometric mean.
        """
        gen_tokens = self._tokenize_code(generated)
        ref_tokens = self._tokenize_code(reference)
        
        if not gen_tokens or not ref_tokens:
            return 0.0
        
        # Calculate unigram precision
        gen_unigrams = gen_tokens
        ref_unigrams = ref_tokens
        
        # Count matches (allow multiple matches per reference token)
        gen_unigram_counts = {}
        for token in gen_unigrams:
            gen_unigram_counts[token] = gen_unigram_counts.get(token, 0) + 1
        
        ref_unigram_counts = {}
        for token in ref_unigrams:
            ref_unigram_counts[token] = ref_unigram_counts.get(token, 0) + 1
        
        # Clip counts at reference count
        unigram_matches = 0
        for token, count in gen_unigram_counts.items():
            unigram_matches += min(count, ref_unigram_counts.get(token, 0))
        
        unigram_precision = unigram_matches / len(gen_unigrams) if gen_unigrams else 0.0
        
        # Calculate bigram precision
        gen_bigrams = [tuple(gen_tokens[i:i+2]) for i in range(len(gen_tokens) - 1)]
        ref_bigrams = [tuple(ref_tokens[i:i+2]) for i in range(len(ref_tokens) - 1)]
        
        if not gen_bigrams:
            return unigram_precision
        
        gen_bigram_counts = {}
        for bigram in gen_bigrams:
            gen_bigram_counts[bigram] = gen_bigram_counts.get(bigram, 0) + 1
        
        ref_bigram_counts = {}
        for bigram in ref_bigrams:
            ref_bigram_counts[bigram] = ref_bigram_counts.get(bigram, 0) + 1
        
        bigram_matches = 0
        for bigram, count in gen_bigram_counts.items():
            bigram_matches += min(count, ref_bigram_counts.get(bigram, 0))
        
        bigram_precision = bigram_matches / len(gen_bigrams) if gen_bigrams else 0.0
        
        # Geometric mean
        if unigram_precision == 0 or bigram_precision == 0:
            return 0.0
        
        geometric_mean = (unigram_precision * bigram_precision) ** 0.5
        
        # Brevity penalty (BP)
        if len(gen_tokens) >= len(ref_tokens):
            bp = 1.0
        else:
            bp = (len(ref_tokens) / len(gen_tokens)) ** 0.5
        
        ng_score = geometric_mean * bp
        return min(1.0, max(0.0, ng_score)) 
    
    def _is_identifier(self, token: str, language: str) -> bool:
        """
        Check if a token is an identifier (variable name, function parameter, etc.).
        Identifiers are alphanumeric tokens that are not keywords.
        """
        # Common keywords that are NOT identifiers
        keywords = {
            'def', 'class', 'if', 'else', 'elif', 'for', 'while', 'return',
            'import', 'from', 'try', 'except', 'with', 'as', 'in', 'is',
            'and', 'or', 'not', 'True', 'False', 'None', 'pass', 'break',
            'continue', 'lambda', 'yield', 'assert', 'del', 'global', 'nonlocal',
            'public', 'private', 'protected', 'static', 'void', 'int', 'string',
            'bool', 'float', 'double', 'char', 'function', 'var', 'let', 'const',
            'async', 'await', 'new', 'this', 'super', 'extends', 'implements'
        }
        
        # Identifiers are alphanumeric (and underscores) that are not keywords
        if not re.match(r'^[a-zA-Z_][a-zA-Z0-9_]*$', token):
            return False
        
        if token in keywords:
            return False
        
        # Exclude punctuation and operators
        if token in ['(', ')', '[', ']', '{', '}', ',', ':', ';', '+', '-', '*', '/', '=', '==', '!=', '<', '>', '<=', '>=']:
            return False
        
        return True
    
    def _calculate_weighted_ngram(self, generated: str, reference: str, language: str) -> float:
        """
        Calculate WNG (weighted n-gram score) - identifier-aware.
        Gives higher weight to identifiers (variables, parameters) as per PDF.
        
        As per PDF: identifiers get weight 2.0, other tokens get weight 1.0.
        """
        gen_tokens = self._tokenize_code(generated)
        ref_tokens = self._tokenize_code(reference)
        
        if not gen_tokens or not ref_tokens:
            return 0.0
        
        # Calculate token weights
        IDENTIFIER_WEIGHT = 2.0
        DEFAULT_WEIGHT = 1.0
        
        # Count matches with weights
        gen_token_weights = {}
        for token in gen_tokens:
            weight = IDENTIFIER_WEIGHT if self._is_identifier(token, language) else DEFAULT_WEIGHT
            if token not in gen_token_weights:
                gen_token_weights[token] = {'weight': weight, 'count': 0}
            gen_token_weights[token]['count'] += 1
        
        ref_token_weights = {}
        for token in ref_tokens:
            weight = IDENTIFIER_WEIGHT if self._is_identifier(token, language) else DEFAULT_WEIGHT
            if token not in ref_token_weights:
                ref_token_weights[token] = {'weight': weight, 'count': 0}
            ref_token_weights[token]['count'] += 1
        
        # Calculate weighted matches (clip at reference count)
        matched_weight = 0.0
        for token, token_info in gen_token_weights.items():
            if token in ref_token_weights:
                matches = min(token_info['count'], ref_token_weights[token]['count'])
                matched_weight += matches * token_info['weight']
        
        # Calculate total candidate weight
        candidate_total_weight = sum(
            info['count'] * info['weight'] 
            for info in gen_token_weights.values()
        )
        
        if candidate_total_weight == 0:
            return 0.0
        
        # Weighted precision
        wng_score = matched_weight / candidate_total_weight
        return min(1.0, max(0.0, wng_score))
    
    def _dataflow_match_score(self, generated: str, reference: str, language: str) -> float:
        """
        Calculate DF (data-flow match score) by comparing data flow patterns.
        
        As per PDF: Data-flow looks at how values flow (which variables used, dependency graph).
        If variable names differ but the flow (param → return expression) is identical, score should be high.
        """
        # Extract data flow patterns (how variables are used and flow through code)
        gen_flow = self._extract_dataflow_patterns(generated, language)
        ref_flow = self._extract_dataflow_patterns(reference, language)
        
        if not ref_flow:
            return 1.0 if not gen_flow else 0.0
        
        # Compare data flow patterns (structure of data dependencies)
        # As per PDF: if flow is identical (param → return expression), score should be high (≈ 1.0)
        if len(gen_flow) == len(ref_flow) and gen_flow == ref_flow:
            return 1.0
        
        # Calculate similarity based on pattern overlap
        overlap = len(gen_flow & ref_flow)
        df_score = overlap / len(ref_flow) if ref_flow else 0.0
        
        # If patterns are similar (same structure, different names), boost score
    
        if len(gen_flow) == len(ref_flow) and overlap >= len(ref_flow) * 0.8:
            return min(1.0, df_score * 1.2)  # Boost for high similarity
        
        return min(1.0, max(0.0, df_score))
    
    def _extract_dataflow_patterns(self, code: str, language: str) -> set:
        """
        Extract data flow patterns from code.
        Looks at how variables flow (parameters, assignments, returns, operations).
        """
        patterns = set()
        
        # Extract function parameters
        if language == "python":
            param_matches = re.findall(r'def\s+\w+\s*\(([^)]*)\)', code)
            for params in param_matches:
                param_count = len([p.strip() for p in params.split(',') if p.strip()])
                patterns.add(f"params:{param_count}")
        else:
            param_matches = re.findall(r'\(([^)]*)\)\s*\{', code)
            for params in param_matches:
                param_count = len([p.strip() for p in params.split(',') if p.strip()])
                patterns.add(f"params:{param_count}")
        
        # Check for return statements (data flow to output)
        return_matches = re.findall(r'return\s+([^;\n]+)', code)
        if return_matches:
            patterns.add("return")
            # Check if return uses parameters or operations
            for ret in return_matches:
                if '+' in ret or '-' in ret or '*' in ret or '/' in ret:
                    patterns.add("return:operation")
        
        # Check for assignments (data flow through variables)
        assignments = re.findall(r'(\w+)\s*=', code)
        if assignments:
            patterns.add(f"assignments:{len(assignments)}")
        
        # Check for operations (data transformations)
        operations = re.findall(r'[\w]+\s*[+\-*/]\s*[\w]+', code)
        if operations:
            patterns.add(f"operations:{len(operations)}")
        
        return patterns
    
    def _extract_variables(self, code: str, language: str) -> set:
        """Extract variable names from code."""
        # Simple pattern matching for variable assignments
        if language == "python":
            pattern = r'(\w+)\s*='
        elif language in ["java", "cpp"]:
            pattern = r'(?:int|string|float|double|bool|char)\s+(\w+)\s*='
        else:
            pattern = r'(\w+)\s*='
        
        variables = set(re.findall(pattern, code))
        return variables
    
    def _ast_match_score(self, generated: str, reference: str, language: str) -> float:
        """
        Calculate AST (Abstract Syntax Tree) match score.
        Compares the parsed tree structures and node types.
        If structure is identical (only identifier names differ), score should be high.
        """
        try:
            # Try to use tree-sitter for proper AST comparison
            from tree_sitter import Language, Parser
            
            # Try to load language parser
            try:
                if language == "python":
                    from tree_sitter_python import language as python_language
                    parser = Parser()
                    parser.set_language(python_language())
                elif language == "java":
                    from tree_sitter_java import language as java_language
                    parser = Parser()
                    parser.set_language(java_language())
                elif language in ["cpp", "c++"]:
                    from tree_sitter_cpp import language as cpp_language
                    parser = Parser()
                    parser.set_language(cpp_language())
                else:
                    raise ImportError(f"Tree-sitter parser not available for {language}")
                
                # Parse both code samples
                gen_tree = parser.parse(bytes(generated, "utf8"))
                ref_tree = parser.parse(bytes(reference, "utf8"))
                
                # Compare AST structures by comparing node types (ignoring identifier names)
                gen_nodes = self._extract_ast_structure(gen_tree.root_node)
                ref_nodes = self._extract_ast_structure(ref_tree.root_node)
                
                # Compare structure similarity
                if not ref_nodes:
                    return 1.0 if not gen_nodes else 0.0
                
                # Calculate similarity based on node type overlap
                gen_node_types = set(gen_nodes)
                ref_node_types = set(ref_nodes)
                
                if not ref_node_types:
                    return 1.0 if not gen_node_types else 0.0
                
                overlap = len(gen_node_types & ref_node_types)
                ast_score = overlap / len(ref_node_types)
                
                return min(1.0, max(0.0, ast_score))
            except ImportError:
                # Fallback: compare structural patterns without tree-sitter
                return self._ast_match_fallback(generated, reference, language)
        except Exception:
            # Fallback: compare structural patterns
            return self._ast_match_fallback(generated, reference, language)
    
    def _extract_ast_structure(self, node) -> List[str]:
        """Extract AST node types (structure) from tree-sitter node."""
        structure = []
        
        def traverse(n):
            if n.type not in ['identifier', 'type_identifier', 'string', 'integer', 'float']:
                # Include structural nodes, ignore identifier names
                structure.append(n.type)
            for child in n.children:
                traverse(child)
        
        traverse(node)
        return structure
    
    def _ast_match_fallback(self, generated: str, reference: str, language: str) -> float:
        """
        Fallback AST matching based on structural similarity.
        Compares code structure patterns (function definitions, control flow, etc.)
        """
        # Extract structural elements (functions, classes, control structures)
        gen_structure = self._extract_structure_patterns(generated, language)
        ref_structure = self._extract_structure_patterns(reference, language)
        
        if not ref_structure:
            return 1.0 if not gen_structure else 0.0
        
        # Compare structural patterns
        overlap = len(gen_structure & ref_structure)
        ast_score = overlap / len(ref_structure) if ref_structure else 0.0
        
        # If structures are very similar, boost score (as per PDF: perfect structural match = 1.0)
        # The PDF shows that even when identifiers differ, if structure is identical, AST = 1.0
        if len(gen_structure) == len(ref_structure) and overlap == len(ref_structure):
            return 1.0
        
        return min(1.0, max(0.0, ast_score))
    
    def _extract_structure_patterns(self, code: str, language: str) -> set:
        """Extract structural patterns from code (functions, classes, control flow)."""
        patterns = set()
        
        # Function definitions
        if language == "python":
            func_matches = re.findall(r'def\s+\w+\s*\([^)]*\)\s*:', code)
            patterns.update([f"func:{len(m)}" for m in func_matches])
            
            # Control structures
            control_structures = ['if', 'elif', 'else', 'for', 'while', 'try', 'except', 'with']
            for struct in control_structures:
                if re.search(rf'\b{struct}\b', code):
                    patterns.add(f"control:{struct}")
        else:
            # For other languages, extract similar patterns
            func_matches = re.findall(r'\w+\s*\([^)]*\)\s*\{', code)
            patterns.update([f"func:{len(m)}" for m in func_matches])
            
            control_structures = ['if', 'else', 'for', 'while', 'try', 'catch']
            for struct in control_structures:
                if re.search(rf'\b{struct}\b', code):
                    patterns.add(f"control:{struct}")
        
        # Class definitions
        class_matches = re.findall(r'class\s+\w+', code)
        patterns.update([f"class" for _ in class_matches])
        
        # Return statements
        if 'return' in code:
            patterns.add("return")
        
        return patterns
    
    def get_evaluation_report(self, generated_code: str, reference_code: str, 
                             language: str = "python") -> str:
        """
        Get a formatted evaluation report.
        
        Args:
            generated_code: Generated code
            reference_code: Reference code
            language: Programming language
        
        Returns:
            Formatted report string
        """
        results = self.evaluate(generated_code, reference_code, language)
        
        report = f"""
CodeBLEU Evaluation Report
==========================
Overall CodeBLEU Score: {results['codebleu']:.4f}
Correctness: {'CORRECT' if results['is_correct'] else 'INCORRECT'}

Component Scores (as per PDF formula):
- NG (n-gram BLEU): {results['ng']:.4f}
- WNG (weighted n-gram): {results['wng']:.4f}
- AST (syntax/AST match): {results['ast']:.4f}
- DF (data-flow match): {results['df']:.4f}

Formula: CodeBLEU = 0.25 × NG + 0.25 × WNG + 0.25 × AST + 0.25 × DF
Threshold: Code is considered correct if CodeBLEU >= 0.75
"""
        return report